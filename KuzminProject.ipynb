{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:52:58.439819Z",
     "start_time": "2025-07-16T20:52:58.398972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the repo root (two levels up from this notebook) to sys.path\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "    print(f\"{repo_root} added to sys.path.\")\n",
    "else:\n",
    "    print(\"Repo root already in sys.path.\")"
   ],
   "id": "43424f64256f4ea5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users added to sys.path.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:53:03.076407Z",
     "start_time": "2025-07-16T20:53:01.218228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from courselib.utils.splits import train_test_split\n",
    "\n",
    "def generateData(n, d, sigma, dense=True):\n",
    "    np.random.seed(52)\n",
    "    X = np.random.normal(0, 1, size=(n, d))\n",
    "\n",
    "    if dense:\n",
    "        beta_star = np.random.randn(d)\n",
    "\n",
    "    else:\n",
    "        beta_star = np.zeros(d)\n",
    "        nonzero_indices = np.random.choice(d, size=10, replace=False)\n",
    "        beta_star[nonzero_indices] = np.random.randn(10)\n",
    "\n",
    "\n",
    "    epsilon = np.random.normal(0, sigma, n)\n",
    "    y = X @ beta_star + epsilon\n",
    "\n",
    "    df = pd.DataFrame(X, columns=[f\"x{i}\" for i in range(X.shape[1])])\n",
    "    df[\"Class\"] = y\n",
    "\n",
    "    X, Y, train_X, train_Y, test_X, test_Y = train_test_split(\n",
    "        df, training_data_fraction=0.8, return_numpy=True\n",
    "    )\n",
    "\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n"
   ],
   "id": "1032e349ed89cb66",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:53:06.264072Z",
     "start_time": "2025-07-16T20:53:06.220579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from courselib.models.linear_models import LinearRegression\n",
    "class RidgeLinearRegression(LinearRegression):\n",
    "\n",
    "    def __init__(self, w, b, optimizer, lam=0.1):\n",
    "        super().__init__(w, b, optimizer)\n",
    "        self.lam = lam\n",
    "\n",
    "    def loss_grad(self, X,Y):\n",
    "        residual = self.decision_function(X) - Y\n",
    "        grad_w = X.T@residual/X.shape[0] + 2*self.lam*self.w\n",
    "        grad_b = np.mean(residual, axis=0)\n",
    "        return {\"w\": grad_w, \"b\": grad_b}"
   ],
   "id": "630533cc8196063a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:53:12.193914Z",
     "start_time": "2025-07-16T20:53:09.347458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "from courselib.models.linear_models import LinearRegression\n",
    "from courselib.optimizers import GDOptimizer\n",
    "from courselib.utils.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_openml\n",
    "from courselib.utils.splits import train_test_split_np\n",
    "from courselib.utils.preprocessing import labels_encoding\n",
    "\n",
    "def computeMSEs(n, range_for_d, sigma, epochs, dense = True, mnist = False, ridge = False):\n",
    "    np.random.seed(52)\n",
    "    random.seed(52)\n",
    "    MSE_train_list = []\n",
    "    MSE_test_list = []\n",
    "    metrics_dict = {'MSE': mean_squared_error}\n",
    "\n",
    "    if mnist:\n",
    "        X_mnist, Y_mnist = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "        X_mnist = X_mnist / 255.0\n",
    "        Y_mnist = Y_mnist.astype(float)\n",
    "        training_data_fraction = .8\n",
    "        X_train_mnist, Y_train_mnist, X_test_mnist, Y_test_mnist = train_test_split_np(X_mnist, Y_mnist, training_data_fraction)\n",
    "\n",
    "    for d in range_for_d:\n",
    "        if mnist:\n",
    "            dimension_index_d = random.sample(range(784), d)\n",
    "            train_indices = random.sample(range(X_train_mnist.shape[0]), int(training_data_fraction * n))\n",
    "            test_indices = random.sample(range(X_test_mnist.shape[0]), int(training_data_fraction * n))\n",
    "            train_X = X_train_mnist[train_indices][:, dimension_index_d]\n",
    "            train_Y = Y_train_mnist[train_indices]\n",
    "            test_X = X_test_mnist[test_indices][:, dimension_index_d]\n",
    "            test_Y = Y_test_mnist[test_indices]\n",
    "\n",
    "        else:\n",
    "            train_X, train_Y, test_X, test_Y = generateData(n, d, sigma, dense)\n",
    "\n",
    "        w = np.zeros(train_X.shape[1])\n",
    "        b = 0\n",
    "        optimizer = GDOptimizer(learning_rate=1e-2)\n",
    "\n",
    "        if ridge:\n",
    "            model = RidgeLinearRegression(w, b, optimizer, lam=0.1)\n",
    "            train_metrics = model.fit(\n",
    "                train_X, train_Y,\n",
    "                num_epochs=epochs,\n",
    "                batch_size=len(train_X),\n",
    "                compute_metrics=True,\n",
    "                metrics_dict=metrics_dict\n",
    "            )\n",
    "            MSE_train = np.mean(train_metrics[\"MSE\"])\n",
    "            MSE_test = mean_squared_error(model.decision_function(test_X), test_Y)\n",
    "\n",
    "            MSE_train_list.append(MSE_train)\n",
    "            MSE_test_list.append(MSE_test)\n",
    "\n",
    "        else:\n",
    "            model = LinearRegression(w, b, optimizer)\n",
    "\n",
    "            train_metrics = model.fit(\n",
    "                train_X, train_Y,\n",
    "                num_epochs=epochs,\n",
    "                batch_size=len(train_X),\n",
    "                compute_metrics=True,\n",
    "                metrics_dict=metrics_dict\n",
    "            )\n",
    "            MSE_train = np.mean(train_metrics[\"MSE\"])\n",
    "            MSE_test = mean_squared_error(model.decision_function(test_X), test_Y)\n",
    "\n",
    "            MSE_train_list.append(MSE_train)\n",
    "            MSE_test_list.append(MSE_test)\n",
    "            cond_number = np.linalg.cond(train_X)\n",
    "\n",
    "    return MSE_train_list, MSE_test_list, cond_number\n"
   ],
   "id": "d91a72405ab217",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T20:53:16.274983Z",
     "start_time": "2025-07-16T20:53:16.225808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def runExperiment(numberOfSamples, rangeForDim, sigma, epochs, isDense, isMnistData, isRidge):\n",
    "    MSE_train, MSE_test, cond_number = computeMSEs(\n",
    "        n=numberOfSamples,\n",
    "        range_for_d=rangeForDim,\n",
    "        sigma=sigma,\n",
    "        epochs=epochs,\n",
    "        dense=isDense,\n",
    "        mnist=isMnistData,\n",
    "        ridge=isRidge\n",
    "    )\n",
    "\n",
    "\n",
    "        # Build descriptive label text\n",
    "    if isMnistData:\n",
    "        info_text = (\n",
    "            f\"MNIST dataset, \"\n",
    "            f\"n = {numberOfSamples}, \"\n",
    "            f\"{'ridge' if isRidge else 'linear'}\"\n",
    "        )\n",
    "    else:\n",
    "        info_text = (\n",
    "            f\"n = {numberOfSamples}, \"\n",
    "            f\"σ = {sigma} \" +\n",
    "            (\"- low noise\" if sigma < 1 else \"- medium noise\" if sigma < 3 else \"- high noise\") + \", \"\n",
    "            f\"d ∈ [{min(rangeForDim)}, {max(rangeForDim)}], \"\n",
    "            f\"{'dense' if isDense else 'not dense'}, \"\n",
    "            f\"synthetic data, \"\n",
    "            f\"{'ridge' if isRidge else 'linear'}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    print(f\"Condition number of design matrix: {cond_number:.2e}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(rangeForDim, MSE_test, label='Test MSE', color='blue')\n",
    "    ax.plot(rangeForDim, MSE_train, label='Train MSE', color='green')\n",
    "    ax.axvline(x=numberOfSamples, linestyle='--', color='red', label='d = n')\n",
    "\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_xlabel('Model Dimension (d)')\n",
    "\n",
    "    plt.title('Double Descent Curve\\n' + info_text)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "d4b331a35723bb95",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-16T20:55:06.555893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "runExperiment(100, range(50,300), 1 , 1000, True, False, False)\n"
   ],
   "id": "94edd6da46f3f365",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
